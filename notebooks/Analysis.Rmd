---
title: "Analysis"
author: "Travis Deason"
date: "2/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
rm( list = ls()); cat("\014")  # Clear environment
set.seed(42)
#install.packages("glmnet")
library(glmnet)
require(stringr)
library(multcomp)
require(plotly)
require(Hmisc)
require(tidyr)
require(dplyr)
require(ggplot2)
require(plyr)
require(reshape2)
source('../src/r_functions.R')
data <- read.csv('../data/train.csv')
test_data <- read.csv('../data/test.csv')
names(data) <- sapply(names(data), tolower)
names(test_data) <- sapply(names(test_data), tolower)
```

```{r}
plot(saleprice~lotarea, data=data)
plot(saleprice~yearremodadd, data=data)
plot(saleprice~wooddecksf, data=data)
plot(saleprice~x1stflrsf, data=data)
plot(saleprice~garagearea, data=data)
plot(saleprice~x2ndflrsf, data=data)
```




```{r}
test_data[,'saleprice'] <- 1
all_data <- rbind.data.frame(data, test_data)

all_data$mssubclass <- factor(all_data$mssubclass)
all_data$mosold <- factor(all_data$mosold)
all_data$has_remodadd <- all_data$yearremodadd != all_data$yearbuilt
#all_data$totporchar <- all_data$wooddecksf + all_data$openporchsf + all_data$enclosedporch + all_data$x3ssnporch + all_data$screenporch

all_data <- fill_nulls(all_data)

all_data$saleprice <- log(all_data$saleprice + 1)
all_data$grlivarea <- log(all_data$grlivarea + 1)
all_data$lotarea <- log(all_data$lotarea + 1)
all_data$x1stflrsf <- log(all_data$x1stflrsf + 1)
all_data$has_2ndflrsf <- all_data$x2ndflrsf != 0
all_data$x2ndflrsf <- log(all_data$x2ndflrsf + 1)

all_data$bsmtunfsf <- log(all_data$bsmtunfsf + 1)
all_data$bsmtfinsf1 <- log(all_data$bsmtfinsf1 + 1)
#all_data$has_totalbsmtsf <- all_data$totalbsmtsf != 0
all_data$totalbsmtsf <- log(all_data$totalbsmtsf + 1)
#all_data$has_garagearea <- all_data$garagearea != 0
all_data$garagearea <- log(all_data$garagearea + 1)
#all_data$has_wooddecksf <- all_data$wooddecksf != 0
all_data$wooddecksf <- log(all_data$wooddecksf + 1)
#all_data$has_openporchsf <- log(all_data$openporchsf != 0)
all_data$openporchsf <- log(all_data$openporchsf +1)
```


```{r}
plot(saleprice~lotarea, data=subset(all_data, saleprice > 1 & lotarea > 0))
plot(saleprice~yearremodadd, data=subset(all_data, saleprice > 1 & has_remodadd == TRUE))
plot(saleprice~wooddecksf, data=subset(all_data, saleprice > 1 & wooddecksf > 0))
plot(saleprice~x1stflrsf, data=subset(all_data, saleprice > 1 & x1stflrsf > 0))
plot(saleprice~garagearea, data=subset(all_data, saleprice > 1 & garagearea > 0))
plot(saleprice~x2ndflrsf, data=subset(all_data, saleprice > 1 & x2ndflrsf > 0))
```


```{r}
factors = 0
not_factors = 0
for(col in names(data)){
    if(is(data[,col][1]) == 'factor'){factors = factors + 1}
    else{not_factors = not_factors + 1}
}
print(factors)
print(not_factors)
print(sum(is.na(data)))
```


* This model should be formed to facilitate the easy interpretation of parameters for use in helping real estate agents, contractors and prospective buyers gain insight into the important factors that influence housing prices in Ames, Iowa.  Characteristics such as ease of measurement and interpretability should be considered.  As part of the report, confidence intervals should be included.  At the minimum, the model should contain at least one continuous and one categorical variable (interaction may be interesting but not required.)  In addition, model selection techniques may be used and an external cross validation should be conducted to compare competing models (in addition to criterion such as the adjusted R2, AIC, BIC, etc.  Finally, make sure and address multicollinearity issues (VIF, etc.) ( You may not have any multicollinearity issues due to the simplicity of your model.   Note: every group will probably come up with different models here.  Your goal is to simply come up with a valid model, whose parameter estimates yield some useful information to your audience.  This can be a considerably less sophisticated (more parsimonious (less factors / predictors) model than in question 2.    Note 2: Your group is limited to only the techniques we have learned in 6371 and up to Unit 6 of 6372

## By looking at all of the variables and identifying those which appear to have the highest impact on salesprice, We have identified the following variables.

* landcontour
* lotarea
* neighborhood
* bldgtype
* overallqual
* has_remodadd
* roofstyle
* masvnrarea
* exterqual
* foundation
* bsmtqual
* heatingqc
* x1stflrsf
* x2ndflrsf
* fullbath
* halfbath
* bedroomabvgr
* fireplacequal
* yearremodadd
* garageyrblt
* garagecars
* totporchar

```{r}
#cols <- c('landcontour', 'lotarea', 'neighborhood', 'bldgtype', 'overallqual', 'has_remodadd', 'roofstyle', 'masvnrarea', 'masvnrarea_isNA', 'exterqual', 'foundation', 'bsmtqual', 'heatingqc', 'x1stflrsf', 'x2ndflrsf', 'fullbath', 'halfbath', 'bedroomabvgr', 'fireplacequ', 'yearremodadd', 'garageyrblt', 'garageyrblt_isNA', 'garagecars', 'totporchar', 'saleprice')

# simple model
sub_frame <- subset(all_data, saleprice > 1)
#model <- lm(saleprice ~landcontour+lotarea+neighborhood+bldgtype+overallqual+foundation+bsmtqual+heatingqc+x1stflrsf+x2ndflrsf+fullbath+bedroomabvgr+fireplacequ+yearremodadd+garagecars, data=sub_frame)
model <-  lm(saleprice ~ neighborhood + grlivarea + overallqual + overallcond + yearbuilt, data=sub_frame)
summary(model)

```

* Observing the model with 1390 degrees of freedom, we have an adjusted R squared of .8743, so this model seems to explain much of the variance fairly well, but since we used 17 columns on 1460 data points, we may have overfit the current dataset.

```{r}
length(model$coefficients)
exp(sort(model$coefficients, decreasing =TRUE))
```


```{r}

```



```{r}
plot(model$residuals)

## plot scatterplot of 1st floor square footage vs sale price
mod_2 <- lm(saleprice~x1stflrsf, data=sub_frame)
pred.int =  predict(mod_2, interval="prediction")
conf.int =  predict(mod_2, interval="confidence")
sub_frame$pred.lower <- pred.int[,2]
sub_frame$pred.upper <- pred.int[,3]

sub_frame$ci.upper <- conf.int[,2]
sub_frame$ci.lower <- conf.int[,3]

slope <- mod_2$coefficients[2]
intercept <- mod_2$coefficients[1]

ggplot(data=sub_frame, aes(x=x1stflrsf, y=saleprice, main='Scatterplot of 1st floor square footage vs saleprice')) + 
    geom_point(color= 'red') +
    geom_abline(intercept=intercept, slope=slope, color='black', size=.2) +
    geom_ribbon(data=sub_frame, aes(ymin= pred.lower, ymax= pred.upper), fill = "blue", alpha = 0.2) +
    geom_ribbon(data=sub_frame, aes(ymin= ci.lower, ymax= ci.upper), fill = "violet", alpha = 0.3)

## plot scatterplot of lot area vs sale price
mod_3 <- lm(saleprice~lotarea, data=sub_frame)
pred.int =  predict(mod_3, interval="prediction")
conf.int =  predict(mod_3, interval="confidence")
sub_frame$pred.lower <- pred.int[,2]
sub_frame$pred.upper <- pred.int[,3]

sub_frame$ci.upper <- conf.int[,2]
sub_frame$ci.lower <- conf.int[,3]

slope <- mod_3$coefficients[2]
intercept <- mod_3$coefficients[1]

ggplot(data=sub_frame, aes(x=lotarea, y=saleprice, main='Scatterplot of lot square footage vs saleprice')) + 
    geom_point(color= 'red') +
    geom_abline(intercept=intercept, slope=slope, color='black', size=.2) +
    geom_ribbon(data=sub_frame, aes(ymin= pred.lower, ymax= pred.upper), fill = "blue", alpha = 0.2) +
    geom_ribbon(data=sub_frame, aes(ymin= ci.lower, ymax= ci.upper), fill = "violet", alpha = 0.3)

#Scatterplot of Neighborhood  vs saleprice
ggplot(data=sub_frame, aes(x=neighborhood, y=exp(saleprice), main='Scatterplot of Neighborhood  vs saleprice')) + 
    geom_point(color= 'red') + theme(axis.text.x = element_text(angle = 90, hjust = 1))

#Scatterplot of Fireplacequal  vs saleprice
ggplot(data=sub_frame, aes(x=fireplacequ, y=exp(saleprice), main='Scatterplot of Fireplacequal  vs saleprice')) + 
    geom_point(color= 'red') + theme(axis.text.x = element_text(angle = 90, hjust = 1))

#Scatterplot of Foundation  vs saleprice
ggplot(data=sub_frame, aes(x=foundation, y=exp(saleprice), main='Scatterplot of Foundation  vs saleprice')) + 
    geom_point(color= 'red') + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

* The first thing we observe while looking at the residuals is that they appear to be normally distributed and of equal varaince for all points along the axis.  This is an indication that a linear regression is suitable for the model as is, but since we have 24 columns, there is a good chance that there is some covariance within the dataset.


```{r}
# chart covariance of continous variables
model <- summary(lm(saleprice ~ neighborhood * grlivarea * overallqual * overallcond * yearbuilt, data=sub_frame))
pvals = coef(model)[,4]
sort(pvals[pvals < .05], decreasing=FALSE)[1:20]
```


* looking at covariance between many of the numeric columns, it is clear that there is a good amount of covariance between the variables.  In order to counter this, instead of going through all catagories and deciding which one to keep on a case by case basis, we will use ridge regression to counter the covariance.


```{r}
## prepare data for use with glmnet tools (lasso and ridge regression)

#subdata <- dplyr::select(all_data, cols)
subdata <- all_data



subdums <- make_dummy(subdata, sep='_', known_cats=c('salecondition'))

tr <- dplyr::select(subset(subdums, saleprice > 1), -saleprice)
te <- dplyr::select(subset(subdums, saleprice <= 1), -saleprice)

x_tr <- as.matrix(tr)
x_te <- as.matrix(te)
y <- subset(all_data, saleprice > 1)$saleprice
```


```{r}
## Generaating Ridge model
mod <- glmnet(x_tr,
              y,
              alpha=0,
              nlambda=2500,
              lambda.min.ratio=0.00005)
plot(mod)
sort(coef(mod)[,100], decreasing = TRUE)[1:25]
mod$lambda[2300]
```
```{r}
# Validating model using cross validation
cv.out <- cv.glmnet(x_tr, y, alpha=0, nlambda=2500, nfolds=10, lambda.min.ratio=0.00005)
plot(cv.out)
cv.out$lambda.min
coefs <- predict(mod, s=cv.out$lambda.min, type="coefficients")[,1]
sort(coefs, decreasing = TRUE)[1:25]
```

* the cross validated model gives a mse of .1236 which is pretty similar to the in-sample error of .032613, so we are happy with it

* the cross validated model for ridge (alpha = 1) gives a mse of .036 which is larger than the in-sample error of .00622, but overall still looks pretty good.  In fact it looks better then ridge regression



```{r}
pred <- predict(cv.out, x_te, s='lambda.min')

ran <- 1:length(pred) + 1460
pred.df <- data.frame(ran)
pred.df$SalePrice <- exp(pred)
names(pred.df) <- c('Id', 'SalePrice')
head(pred.df)
write.csv(pred.df, '../data/r_ridge_all_vars.csv', row.names = FALSE)
```


```{r}
## Generaating Lasso model
mod <- glmnet(x_tr,
              y,
              alpha=1,
              nlambda=2500,
              lambda.min.ratio=0.00005)
sort(coef(mod)[,1000], decreasing = TRUE)[1:25]
plot(mod)
mod$lambda[1500]
```
```{r}
# Validating model using cross validation
cv.out <- cv.glmnet(x_tr, y, alpha=1, nlambda=2500, nfolds=10, lambda.min.ratio=0.00005)
plot(cv.out)
cv.out$lambda.min
coefs <- predict(mod, s=cv.out$lambda.min, type="coefficients")[,1]
sort(coefs, decreasing = TRUE)[1:25]
```

* the cross validated model gives a mse of .0 which is pretty similar to the in-sample error of .032613, so we are happy with it

* the cross validated model for ridge (alpha = 1) gives a mse of .036 which is larger than the in-sample error of .00622, but overall still looks pretty good.  In fact it looks better then ridge regression



```{r}
pred <- predict(cv.out, x_te, s='lambda.min')

ran <- 1:length(pred) + 1460
pred.df <- data.frame(ran)
pred.df$SalePrice <- exp(pred)
names(pred.df) <- c('Id', 'SalePrice')
head(pred.df)
write.csv(pred.df, '../data/r_lasso_all_vars.csv', row.names = FALSE)
```

