---
title: "Analysis"
author: "Travis Deason"
date: "2/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
rm( list = ls()); cat("\014")  # Clear environment
#install.packages("glmnet")
library(glmnet)
require(stringr)
library(multcomp)
require(plotly)
require(Hmisc)
require(tidyr)
require(dplyr)
require(ggplot2)
require(plyr)
require(reshape2)
source('../src/r_functions.R')
data <- read.csv('../data/train.csv')
test_data <- read.csv('../data/test.csv')
names(data) <- sapply(names(data), tolower)
names(test_data) <- sapply(names(test_data), tolower)
```

```{r}

test_data[,'saleprice'] <- 1
all_data <- rbind.data.frame(data, test_data)

all_data$mssubclass <- factor(all_data$mssubclass)
all_data$has_remodadd <- all_data$yearremodadd != all_data$yearbuilt
all_data <- fill_nulls(all_data)
all_data$totporchar <- all_data$wooddecksf + all_data$openporchsf + all_data$enclosedporch + all_data$x3ssnporch + all_data$screenporch


all_data$saleprice <- log(all_data$saleprice + 1)
all_data$grlivarea <- log(all_data$grlivarea + 1)
all_data$lotarea <- log(all_data$lotarea + 1)
all_data$x1stflrsf <- log(all_data$x1stflrsf + 1)
all_data$x2ndflrsf <- log(all_data$x2ndflrsf + 1)

all_data$bsmtunfsf <- log(all_data$bsmtunfsf + 1)
all_data$bsmtfinsf1 <- log(all_data$bsmtfinsf1 + 1)
all_data$totalbsmtsf <- log(all_data$totalbsmtsf + 1)
all_data$garagearea <- log(all_data$garagearea + 1)
all_data$wooddecksf <- log(all_data$wooddecksf + 1)
all_data$openporchsf <- log(all_data$openporchsf +1)
```



* This model should be formed to facilitate the easy interpretation of parameters for use in helping real estate agents, contractors and prospective buyers gain insight into the important factors that influence housing prices in Ames, Iowa.  Characteristics such as ease of measurement and interpretability should be considered.  As part of the report, confidence intervals should be included.  At the minimum, the model should contain at least one continuous and one categorical variable (interaction may be interesting but not required.)  In addition, model selection techniques may be used and an external cross validation should be conducted to compare competing models (in addition to criterion such as the adjusted R2, AIC, BIC, etc.  Finally, make sure and address multicollinearity issues (VIF, etc.) ( You may not have any multicollinearity issues due to the simplicity of your model.   Note: every group will probably come up with different models here.  Your goal is to simply come up with a valid model, whose parameter estimates yield some useful information to your audience.  This can be a considerably less sophisticated (more parsimonious (less factors / predictors) model than in question 2.    Note 2: Your group is limited to only the techniques we have learned in 6371 and up to Unit 6 of 6372

## By looking at all of the variables and identifying those which appear to have the highest impact on salesprice, We have identified the following variables.

* landcontour
* lotarea
* neighborhood
* bldgtype
* overallqual
* has_remodadd
* roofstyle
* masvnrarea
* exterqual
* foundation
* bsmtqual
* heatingqc
* x1stflrsf
* x2ndflrsf
* fullbath
* halfbath
* bedroomabvgr
* fireplacequal
* yearremodadd
* garageyrblt
* garagecars
* totporchar

```{r}
cols <- c('landcontour', 'lotarea', 'neighborhood', 'bldgtype', 'overallqual', 'has_remodadd', 'roofstyle', 'masvnrarea', 'masvnrarea_isNA', 'exterqual', 'foundation', 'bsmtqual', 'heatingqc', 'x1stflrsf', 'x2ndflrsf', 'fullbath', 'halfbath', 'bedroomabvgr', 'fireplacequ', 'yearremodadd', 'garageyrblt', 'garageyrblt_isNA', 'garagecars', 'totporchar', 'saleprice')

sub_frame <- subset(all_data, saleprice > 1)
#sub_x <- select(sub_frame, cols)
#sub_y <- select(sub_frame, saleprice)
model <- lm(saleprice ~landcontour+lotarea+neighborhood+bldgtype+overallqual+roofstyle+exterqual+foundation+bsmtqual+heatingqc+x1stflrsf+x2ndflrsf+fullbath+halfbath+bedroomabvgr+fireplacequ+yearremodadd+garageyrblt+garageyrblt_isNA+garagecars+totporchar, data=sub_frame)
summary(model)
```


* The first things we observe is that based on 1388 degrees of freedom, we have an adjusted R squared of .8743, so this model seems to explain the variance in the model fairly well, but since we used 24 columnd on 1460 data points, we may have overfit the current dataset.

```{r}
plot(model$residuals)

## plot scatterplot of 1st floor square footage vs sale price
mod_2 <- lm(saleprice~x1stflrsf, data=sub_frame)
pred.int =  predict(mod_2, interval="prediction")
conf.int =  predict(mod_2, interval="confidence")
sub_frame$pred.lower <- pred.int[,2]
sub_frame$pred.upper <- pred.int[,3]
sub_frame$ci.upper <- conf.int[,2]
sub_frame$ci.lower <- conf.int[,3]

slope <- mod_2$coefficients[2]
intercept <- mod_2$coefficients[1]

ggplot(data=sub_frame, aes(x=x1stflrsf, y=saleprice, main='Scatterplot of 1st floor square footage vs saleprice')) + 
    geom_point(color= 'red') +salr
    geom_abline(intercept=intercept, slope=slope, color='black', size=.2) +
    geom_ribbon(data=sub_frame, aes(ymin= pred.lower, ymax= pred.upper), fill = "blue", alpha = 0.2) +
    geom_ribbon(data=sub_frame, aes(ymin= ci.lower, ymax= ci.upper), fill = "violet", alpha = 0.3)
```

* The first thing we observe while looking at the residuals is that they appear to be normally distributed and of equal varaince for all points along the axis.  This is an indication that a linear regression is suitable for the model as is, but since we have 24 columns, there is a good chance that there is some covariance within the dataset.


```{r}
summary(lm(saleprice~overallqual*yearremodadd, data=sub_frame))
summary(lm(saleprice~overallqual*garageyrblt, data=sub_frame))
summary(lm(saleprice~garageyrblt*yearremodadd, data=sub_frame))
summary(lm(saleprice~x1stflrsf*garagearea, data=sub_frame))
summary(lm(saleprice~totporchar*lotarea, data=sub_frame))

```


* looking at covariance between many of the numeric columns, it is clear that there is a good amount of covariance between the variables.  In order to counter this, instead of going through all catagories and deciding which one to keep on a case by case basis, we will use ridge regression to counter the covariance.


```{r}
subdata <- select(all_data, cols)

subdums <- make_dummy(subdata, sep='_', cat_but_keep=c('potato'), known_cats=c('swiss'))

tr <- select(subset(subdums, saleprice > 1), -saleprice)
te <- select(subset(subdums, saleprice <= 1), -saleprice)

x_tr <- as.matrix(tr)
x_te <- as.matrix(te)
y <- subset(all_data, saleprice > 1)$saleprice

mod <- glmnet(x_tr,
              y,
              alpha=0,
              nlambda=1000,
              lambda.min.ratio=0.0001)
coef(mod)[,100]
mod$lambda[1000]
```
```{r}
# Validating model using cross validation
cv.out <- cv.glmnet(x_tr, y, alpha=0, nlambda=1000, nfolds=20, lambda.min.ratio=0.0001)
plot(cv.out)
cv.out$lambda.min
```

* the cross validated model gives a mse of 6398.5 which is pretty similar to the in-sample error of 6281.603, so we are happy with it



```{r}
#coefs <- predict(mod, s=cv.out$lambda.min, type="coefficients")
#subtest <- select(test_data, cols)
#subtest <- make_dummy(subtest, sep='_', cat_but_keep=c('potato'), known_cats=c('swiss'))
#dim(x_te)
#coefs_noint <- length(coefs[2:length(coefs)-1])
#x_te %*% coefs_noint
pred <- predict(cv.out, x_te, s='lambda.min')

ran <- 1:length(pred) + 1460
pred.df <- data.frame(ran)
pred.df$SalePrice <- exp(pred)
names(pred.df) <- c('Id', 'SalePrice')
head(pred.df)
write.csv(pred.df, '../data/r_ridge_model.csv', row.names = FALSE)
```

